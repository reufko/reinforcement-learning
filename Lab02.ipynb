{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f46c6e8a-5345-484d-a33e-255bc2ca0270",
   "metadata": {},
   "source": [
    "# Lab02 - Monte Carlo Methods\n",
    "\n",
    "### Learning Goals:\n",
    "- Getting to know the blackjack environment\n",
    "- Understanding policies in the context of Reinforcement Learning\n",
    "- Understanding Monte Carlo methods and implementing a First-visit MC prediction algorithm\n",
    "- Visualizing the value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e71e89-251a-44b3-9561-51405f894e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "from toolbox.blackjack import BlackjackEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d25218b-f029-463d-994a-88828d7c9c43",
   "metadata": {},
   "source": [
    "## 2.1 Blackjack Environment\n",
    "\n",
    "The object of the popular casino card game of blackjack is to obtain cards the sum of whose numerical values is as great as possible without exceeding 21. All face cards count as 10, and an ace can count either as 1 or as 11. We consider the version in which each player competes independently against the dealer. The game begins with two cards dealt to both dealer and player. One of the dealer’s cards is face up and the other is face down. If the player has 21 immediately (an ace and a 10-card), it is called a natural. He then wins unless the dealer also has a natural, in which case the game is a draw. If the player does not have a natural, then he can request additional cards, one by one (hits), until he either stops (sticks) or exceeds 21 (goes bust). If he goes bust, he loses; if he sticks, then it becomes the dealer’s turn. The dealer hits or sticks according to a fixed strategy without choice: he sticks on any sum of 17 or greater, and hits otherwise. If the dealer goes bust, then the player wins; otherwise, the outcome - win, lose, or draw - is determined by whose final sum is closer to 21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ca64ca-865a-4344-8d8a-59e9f1d8c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlackjackEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5215746b-4109-4a79-a806-7839bb56e350",
   "metadata": {},
   "source": [
    "Playing blackjack is naturally formulated as an episodic finite MDP. Each game of blackjack is an episode. Rewards of +1, -1, and 0 are given for winning, losing, and drawing, respectively. All rewards within a game are zero, and we do not discount ($\\gamma = 1$); therefore these terminal rewards are also the returns. The player’s actions are to hit or to stick. The states depend on the player’s cards and the dealer’s showing card. We assume that cards are dealt from an infinite deck (i.e., with replacement) so that there is no advantage to keeping track of the cards already dealt. If the player holds an ace that he could count as 11 without going bust, then the ace is said to be usable. In this case it is always counted as 11 because counting it as 1 would make the sum 11 or less, in which case there is no decision to be made because, obviously, the player should always hit. Thus, the player makes decisions on the basis of three variables: his current sum (12–21), the dealer’s one showing card (ace–10), and whether or not he holds a usable ace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcd531f-12d0-4627-af1a-b83d8960a69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_observation(observation):\n",
    "    score, dealer_score, usable_ace = observation\n",
    "    # TODO: Print player's score, if the player has a usable ace and the dealer's score\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eb2eb9-959a-447b-bbf5-a6a5d01b87e2",
   "metadata": {},
   "source": [
    "### Defining a policy\n",
    "**TODO:** Define a policy that sticks if the player’s sum is 20 or 21, and otherwise hits in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001d4418-2478-4bad-9b1b-7a51c8dcf705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy(observation):\n",
    "    # TODO: Unpack the observation\n",
    "    ...\n",
    "    # TODO: Stick (action 0) if the score is >= 20, hit (action 1) otherwise\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0366abb1-af81-4692-9569-71483c61820b",
   "metadata": {},
   "source": [
    "### Getting familiar with the environment\n",
    "**TODO:** Write a program that plays 10 games using the blackjack environment imported above and the policy you defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7f81a6-6b68-45e2-aadd-a3919b29b2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Play 10 games using the defined policy and the imported blackjack environment. Print the results.\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e4d3a0-0076-4ff3-b788-0a754336e49e",
   "metadata": {},
   "source": [
    "## 2.2 Monte Carlo Prediction\n",
    "### 2.2.1 First-visit MC prediction\n",
    "\n",
    "In this section we use Monte Carlo methods for learning the state-value function for a given policy. Recall that the value of a state is the expected return, starting from that state. An obvious way to estimate it from experience is to average the returns observed after visits to that state.\n",
    "\n",
    "Suppose we want to estimate $v_\\pi(s)$ (the value of a state $s$ under policy $\\pi$, given a set of episodes obtained by following $\\pi$ and passing through $s$. The **first-visit MC** method estimates $v_\\pi(s)$ as the average of the returns following **first** visits to $s$. \n",
    "\n",
    " \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>MC Policy Evaluation</b>: Given a policy, we want to estimate the state-value function V(s). Sample episodes of experience and estimate V(s) to be the reward received from that state onwards averaged across all of your experience. The same technique works for the action-value function Q(s, a). Given enough samples, this is proven to converge.\n",
    "</div>\n",
    "\n",
    "**TODO:** Implement First-visit MC prediction from Sutton & Barto Chapter 5.1 to evaluate the defined policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39359708-703a-44fc-a207-0d2e31162bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_prediction_first_visit(policy, env, num_episodes, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the value function\n",
    "    for a given policy using sampling.\n",
    "    \n",
    "    Args:\n",
    "        policy: A function that maps an observation to action probabilities.\n",
    "        env: OpenAI gym environment.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps from state -> value.\n",
    "        The state is a tuple and the value is a float.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement first-visit MC prediction from Sutton & Barto Chapter 5.1 to evaluate the defined policy\n",
    "\n",
    "    return V    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c93296-390a-425a-916f-ba7b2f8b7aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run the MC prediction function for 10.000 episodes and\n",
    "V_10k_first_visit = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5937aa52-b47b-49f0-97c5-4a5a5229c3c5",
   "metadata": {},
   "source": [
    "### 2.2.2 Every-visit MC prediction\n",
    "The **every-visit MC** method estimates $v_\\pi(s)$ as the average of the returns following **all** visits to $s$. \n",
    "\n",
    "**TODO:** Sligthly change the first-visit method implemented above to obtain the every-visit mc prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb14ecb5-916e-4c39-8e4b-07775a73f161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_prediction_every_visit(policy, env, num_episodes, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the value function\n",
    "    for a given policy using sampling.\n",
    "    \n",
    "    Args:\n",
    "        policy: A function that maps an observation to action probabilities.\n",
    "        env: OpenAI gym environment.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps from state -> value.\n",
    "        The state is a tuple and the value is a float.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement every-visit MC prediction from Sutton & Barto Chapter 5.1 to evaluate the defined policy\n",
    "\n",
    "    return V    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af32b061-e63a-4615-9d66-79e4eb6bfa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run the MC prediction function for 10.000 episodes and\n",
    "V_10k_every_visit = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc3912a-08a8-47c1-aa4e-8e530043c4a8",
   "metadata": {},
   "source": [
    "## 2.3 Visualizing of the Value Function\n",
    "In this section you will learn how to visualize properly the value function of the evaluated policy. In the following you can see one possible way of value function visualization. \n",
    "\n",
    "![value_function](images/Ex2.3_plot.png)\n",
    "\n",
    "**TODO:** Try to recreate the given plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d501fc5-c49b-4650-8e3d-386e84a60cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_value_function(V, title=\"Value Function\"):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64524c53-c9c1-433c-b439-3dad41c86ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_value_function(V_10k_first_visit, title=\"10,000 Steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb438de3-3a62-4357-94e0-806c7554dccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_value_function(V_10k_every_visit, title=\"10,000 Steps\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

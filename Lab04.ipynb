{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f46c6e8a-5345-484d-a33e-255bc2ca0270",
   "metadata": {},
   "source": [
    "# Lab04 - Temporal-Difference Learning\n",
    "\n",
    "### Learning Goals:\n",
    "- Getting familiar with the cliff walking environment\n",
    "- Understanding Temporal-Difference Learning in particular Q-Learning\n",
    "- Understanding the difference to SARSA\n",
    "- Visualizing the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e71e89-251a-44b3-9561-51405f894e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "from toolbox.cliff_walking import CliffWalkingEnv\n",
    "import sys\n",
    "import itertools\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed945ae1-1864-4926-af4c-b055062e373f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4.1 Cliff Walking Environment\n",
    "Consider the gridworld shown below. This is a standard undiscounted, episodic task, with start and goal states, and the usual actions causing movement up, down, right, and left. Reward is -1 on all transitions except those into the region marked \"The Cliff\". Stepping into this region incurs a reward of -100 and sends the agent instantly back to the start.\n",
    "\n",
    "<div>\n",
    "<img src=\"images/Ex4.1_cliff_walking.png\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "**TODO:** Take a few steps inside the cliff walking environment and get familiar with it. Try the functions `reset()`, `step()` and `render()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8c6feef-20c9-4e84-807a-ec1bed911037",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CliffWalkingEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5996a0-e574-4558-9d27-8be07069c91a",
   "metadata": {},
   "source": [
    "## 4.2 Q-Learning (Off-policy TD Control)\n",
    "\n",
    "Temporal-difference (TD) learning is undoubtedly a central idea for reinforcement learning. It is a combination of Monte Carlo ideas and DP ideas. Monte Carlo methods: learning from experience without knowing the model. Dynamic programming: update estimate based on other learned estimates.\n",
    "\n",
    "**Advantage:** Opposite to DP methods, TD methods do not require a model of the environment, which is very helpful. They also can be implemented in incremental fashion, not needing to wait for the end of episodes, like it is the case with Monte Carlo methods. Learning from one guess to another without waiting for the final outcome is very convenient and in TD case it has also been proven to converge. \n",
    "\n",
    "General Update Rule: `Q[s,a] += learning_rate * (td_target - Q[s,a])`. \n",
    "\n",
    "TD Error: `td_target - Q[s,a]`\n",
    "\n",
    "TD Target for Q-Learning: `R[t+1] + discount_factor * max(Q[next_state])`\n",
    "\n",
    "**TODO:** Implement the Q-learning (off-policy TD control) from Sutton & Barto Chapter 6.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aace94-1645-48e4-b56f-cd42596af57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action(Q, observation, nA, epsilon):\n",
    "    \"\"\"\n",
    "    Chooses an epsilon-greedy action based on the state action function, observation and current epsilon value.\n",
    "    Args:\n",
    "        Q: Dictionary mapping state -> action values.\n",
    "        observation: Tuple (state, action, reward).\n",
    "        nA: Number of possible actions in the given environment.\n",
    "        epsilon: Current epsilon value.\n",
    "        \n",
    "    Returns:\n",
    "        action: The chosen action.\n",
    "    \"\"\"\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d54493-defe-4282-8f5d-2cf606d83a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_new_Q(Q, reward, discount_factor, state, next_state, current_action):\n",
    "    \"\"\"\n",
    "    Calculates the new Q value based on the given parameters.\n",
    "    \n",
    "    Args:\n",
    "        Q: Dictionary mapping state -> action values.\n",
    "        reward: Current reward achieved from the last step.\n",
    "        discount_factor: float representing the discount factor to be used in the Q-learning algorithm.\n",
    "        state: Current state the agent is in.\n",
    "        next_state: The state after the step taken.\n",
    "        current_action: Action agent has taken.\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a15df0-4352-43bb-bc4e-c6fc715cdea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Implementation of the Q-learning algorithm. \n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        num_episodes: This is an integer representing the number of episodes of interaction with the environment that the Q-learning algorithm should run for. \n",
    "        discount_factor: This is a float representing the discount factor to be used in the Q-learning algorithm.\n",
    "        alpha: This is a float representing the learning rate to be used in the Q-learning algorithm.\n",
    "        epsilon: This is a float representing the exploration rate to be used in the Q-learning algorithm. \n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, stats).\n",
    "        Q is a dictionary mapping state -> action values.\n",
    "        episode_lengths is an array holding the lengths of each episode (how many steps have been taken)\n",
    "        episode_rewards is an array holding the achieved reward of each episode\n",
    "    \"\"\"\n",
    "    \n",
    "    # The final action-value function.\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    Q = ...\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    episode_lengths = ...\n",
    "    episode_rewards = ...\n",
    "        \n",
    "    # Loop through episodes\n",
    "    for ...:\n",
    "        # Print out which episode we're on, useful for debugging.\n",
    "        if (i_episode + 1) % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode + 1, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        # Reset the environment\n",
    "        state = ...\n",
    "        \n",
    "        # Take steps until termination state\n",
    "        while ...:\n",
    "            \n",
    "            # Take a step using your `epsilon_greedy_action()` function\n",
    "            action = ...\n",
    "            next_state, reward, done, _ = ...\n",
    "            \n",
    "            # Update your Q table according to the current knowledge\n",
    "            calculate_new_Q(...)\n",
    "            \n",
    "            # Update your statistics \n",
    "            episode_lengths = ...\n",
    "            episode_rewards = ...\n",
    "            \n",
    "            # Break the loop if the episode is done\n",
    "            if ...:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "    \n",
    "    return Q, episode_lengths, episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3198e00c-f482-4dfd-b6a7-b24cc4021722",
   "metadata": {},
   "source": [
    "## 4.3 Visualizing the training process\n",
    "Use the created statistics inside your `episode_lengths` and `episode_rewards` in order to visualize the training process of your agent. \n",
    "\n",
    "**TODO:**\n",
    "- First visualization should be a plot of the episode lengths over episodes. Your x-axis displays the current episode, while y-axis is the length (number of steps the agent has taken during the episode.\n",
    "- Second visualization should be a plot of the achieved revard of episodes. Your x-axis displays the current episode, whilye y-axis shows the reward achieved during that episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc14f89-92d3-4d1f-be90-1e7916c3f519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_episode_stats(episode_lengths, episode_rewards):\n",
    "    \"\"\"\n",
    "    Plotting the statistics of the training process.\n",
    "    Args:\n",
    "        episode_lengths is an array holding the lengths of each episode (how many steps have been taken)\n",
    "        episode_rewards is an array holding the achieved reward of each episode\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c597e5a-b8d5-4c9d-9fba-cf8364bb9e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, episode_lengths, episode_rewards = q_learning(env, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9164ad49-db66-4489-8950-9894d8bc9cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_episode_stats(episode_lengths, episode_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e665b1ad-96f4-4975-aad4-af13d2e47959",
   "metadata": {},
   "source": [
    "## 4.4 SARSA (On-policy TD-Control)\n",
    "The basic idea behind Sarsa is to learn the action-value function $Q(s,a)$ that estimates the expected long-term reward for taking a given action a in a given state s. This is done by using a trial-and-error approach, where the algorithm tries different actions in different states and updates its estimates based on the rewards that it receives.\n",
    "\n",
    "The Sarsa algorithm uses an exploration-exploitation trade-off, where it balances the need to explore new actions and states with the need to exploit the current knowledge of the action-value function to take the best known action in each state. This is typically done using an exploration function, such as an epsilon-greedy strategy, which determines the probability of choosing a random action instead of the best known action.\n",
    "\n",
    "Overall, the Sarsa algorithm is a simple and effective method for learning action values in reinforcement learning problems. It has been applied to a wide range of problems, including control, game playing, and other tasks.\n",
    "\n",
    "TD Target for SARSA: `R[t+1] + discount_factor * Q[next_state][next_action]`\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    The difference between Q-learning and SARSA is that SARSA chooses an action following the same current policy and updates its Q-values whereas Q-learning chooses the greedy action, that is, the action that gives the maximum Q-value for the state, that is, it follows an optimal policy.\n",
    "</div>\n",
    "\n",
    "**TODO:** Implement the SARSA (on-policy TD control) algorithm from Sutton and Barto Chapter 6.4. Hint: Just a minor changes inside your `q_learning()` function are needed to get the SARSA algorithm working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "804c6465-c4d8-48b8-875b-914f3951b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Implementation of the SARSA algorithm. \n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        num_episodes: This is an integer representing the number of episodes of interaction with the environment that the Q-learning algorithm should run for. \n",
    "        discount_factor: This is a float representing the discount factor to be used in the Q-learning algorithm.\n",
    "        alpha: This is a float representing the learning rate to be used in the Q-learning algorithm.\n",
    "        epsilon: This is a float representing the exploration rate to be used in the Q-learning algorithm. \n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, stats).\n",
    "        Q is a dictionary mapping state -> action values.\n",
    "        episode_lengths is an array holding the lengths of each episode (how many steps have been taken)\n",
    "        episode_rewards is an array holding the achieved reward of each episode\n",
    "    \"\"\"\n",
    "    \n",
    "    return Q, episode_lengths, episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b953ec1-8e6e-4914-8a9d-3b26f73d5dfb",
   "metadata": {},
   "source": [
    "## 4.5 Comparison of Results\n",
    "Compare the results of both algorithms and understand the differences between both of them.\n",
    "\n",
    "**TODO:** Plot the statistics of both algorithms over each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187cf827-83a3-4ced-9e9e-d06e92a67c85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

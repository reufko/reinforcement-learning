{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f46c6e8a-5345-484d-a33e-255bc2ca0270",
   "metadata": {},
   "source": [
    "# Lab03 - Monte Carlo Control\n",
    "\n",
    "### Learning Goals:\n",
    "- Understanding policies in the context of Reinforcement Learning\n",
    "- Understanding Monte Carlo methods and implementing a First-visit MC prediction algorithm\n",
    "- Visualizing the value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e71e89-251a-44b3-9561-51405f894e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "from toolbox.blackjack import BlackjackEnv\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d25218b-f029-463d-994a-88828d7c9c43",
   "metadata": {},
   "source": [
    "## 3.1 Monte Carlo Control\n",
    "\n",
    "Let's consider how Monte Carlo estimation can be used to approximate optimal policies. The overall idea is to maintain both an approximate policy and an approximate value function. The value function is repeatedly altered to more closely approximate the value function for the current policy, and the policy is repeatedly improved with respect to the current value function. \n",
    "\n",
    "To begin let's recall the classical policy iteration. In this method, alternating complete steps of policy evaluation and policy improvement are completed:\n",
    "\n",
    "\\begin{equation*}\n",
    "   \\large \\pi_0 \\xrightarrow[]{E} v_{\\pi_0} \\xrightarrow[]{I} \\pi_1 \\xrightarrow[]{E} v_{\\pi_1} \\xrightarrow[]{I} \\pi_2 \\xrightarrow[]{E} ... \\xrightarrow[]{I} \\pi_* \\xrightarrow[]{E} v_*\n",
    "\\end{equation*}\n",
    "\n",
    "In the Monte Carlo version of classical policy iteration policy evaluation is done exactly like in the preceding section. Many episodes are experienced,  with the approximate action-value function approaching the true function asymptotically. We assume that we observe an infinite number of episodes with exploring starts. Under these assumptions, the Monte Carlo methods will compute each $q_{\\pi_k}$ exactly, for arbitrary $\\pi_k$. \n",
    "\n",
    "Policy improvement is done by making the policy greedy with respect to the current value function. No model is needed, since we have an action-value function. For each $s \\in \\mathcal{S}$ we deterministically choose an action with maximal action-value:\n",
    "\\begin{equation}\n",
    "    \\pi(s) \\doteq \\underset{a}{\\operatorname{arg max}} q(s,a)\n",
    "\\end{equation}\n",
    "\n",
    "Two unlikely assumptions have been made in order to obtain the convergence guarantee. First one, episodes have exploring starts and second, policy evaluation can be done with an infinite number of episodes.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>MC Control:</b> Idea is the same as for Dynamic Programming. Use MC Policy Evaluation to evaluate the current policy then improve the policy greedily.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>The Problem:</b> How do we ensure that we explore all states if we don't know the full environment?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Solution to exploration problem:</b> Use epsilon-greedy policies instead of full greedy policies. When making a decision act randomly with probability epsilon. This will learn the optimal epsilon-greedy policy.\n",
    "</div>\n",
    "\n",
    "**TODO:** Implement the on-policy first-visit MC control (for $\\epsilon$-soft policies) from Sutton & Barto Chapter 5.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ca64ca-865a-4344-8d8a-59e9f1d8c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlackjackEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d7eef3-3513-4d01-b417-a22d269b11bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decayed_epsilon_value(epsilon, current_episode, episodes, epsilon_min = 0.05):\n",
    "    \"\"\"\n",
    "    Calculates the current decayed epsilon value.\n",
    "    Args:\n",
    "        epsilon: The probability to select random action. Float between 0 and 1.\n",
    "        current_episode: Current episode.\n",
    "        episodes: Number of episodes.\n",
    "        epsilon_min: Minimum value epsilon is being decayed to.\n",
    "    Returns:\n",
    "        current_epsilon: The decayed epsilon value.\n",
    "    \"\"\"\n",
    "    # TODO: Calculate the linear decayed value of epsilon\n",
    "    \n",
    "    return current_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7828462-c4d8-438e-815c-d789673f3c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action(Q, observation, nA, epsilon):\n",
    "    \"\"\"\n",
    "    Chooses an epsilon-greedy action based on the action value function, observation and current epsilon value.\n",
    "    Args:\n",
    "        Q: Dictionary mapping state -> action values.\n",
    "        observation: Tuple (state, action, reward).\n",
    "        nA: Number of possible actions in the given environment.\n",
    "        epsilon: Current epsilon value.\n",
    "        \n",
    "    Returns:\n",
    "        action: The chosen action.\n",
    "    \"\"\"\n",
    "    # TODO: Generate an action based on the current action value function.\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d85497-1369-4207-b41e-f90337f70001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, Q, epsilon):\n",
    "    \"\"\"\n",
    "    Generates an episode using an epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        Q: Dictionary mapping state -> action values.\n",
    "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    "    Returns:\n",
    "        episode: List of tuples [(state, action, reward), ...]\n",
    "    \"\"\" \n",
    "    \n",
    "    # TODO: Simulate a game of blackjack and save all the states of the episode into `episode`\n",
    "    \n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad060bd-c090-4123-b985-6fb0c346d211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_new_Q(env, episode, Q, return_stats):\n",
    "    \"\"\"\n",
    "    Updates Q using the most recently generated episode.\n",
    "    \n",
    "    Args:\n",
    "        env: openAI gym environment.\n",
    "        episode: Most recently generated episode.\n",
    "        Q: Dictionary mapping state -> action values.\n",
    "        return_stats: Dictionary holding statistics of states and actions (state, action) -> [sum_of_returns, number_of_visits]\n",
    "    Returns:\n",
    "        Q: Updated action value function table mapping state -> action values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Update your action value function table according to the recently generated episode and pseudo code in the Sutton & Barto Chapter 5.4\n",
    "        \n",
    "    return Q\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa553f0-31b4-465a-b4ba-997b72d630ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_epsilon_greedy(env, num_episodes, epsilon=0.1, epsilon_min = 0.05):\n",
    "    \"\"\"\n",
    "    Monte Carlo Control using Epsilon-Greedy policies.\n",
    "    Finds an optimal epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    "        epsilon_min: Minimum value epsilon is being decayed to.\n",
    "    \n",
    "    Returns:\n",
    "        Q: A dictionary mapping state -> action values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Declare a dictionary which will be holding statistics of states and actions \n",
    "    # return_stats = (state, action) -> [sum_of_returns, number_of_visits]\n",
    "    return_stats = ...\n",
    "    \n",
    "    # TODO: Declare a dictionary which will hold your action-value function.\n",
    "    # Q = state -> (action -> action-value)\n",
    "    Q = ...\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        \n",
    "        # TODO 1: Decay epsilon\n",
    "        \n",
    "        # TODO 2: Use function `generate_episode()` to simulate a whole episode\n",
    "                \n",
    "        # TODO 3: Update your Q\n",
    "\n",
    "        # 4. Monitor progress:\n",
    "        if i_episode % 1000 == 0:\n",
    "            print('\\rEpisode {}/{}.'.format(i_episode, num_episodes), end = \"\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7dce33-b4df-4f5f-aa9e-2298f9d8d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = mc_control_epsilon_greedy(env, num_episodes=500000, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9aca56-cf5a-4f4f-8b7f-ec0f45ad6bcc",
   "metadata": {},
   "source": [
    "## 3.2 Visualizing the Value Function\n",
    "\n",
    "Your function `mc_control_epsilon_greedy()` returns the action-value function $q_\\pi(s,a)$ (`Q`) for the learned $\\epsilon$-greedy policy. Use the generated `Q` to calculate the value function:\n",
    "\n",
    "\\begin{equation*}\n",
    "   \\large v(s) = \\max_a q_\\pi(s,a)\n",
    "\\end{equation*}\n",
    "\n",
    "**TODO:** Calculate the value function and reuse the `plot_value_function()` from the Lab02 to visualize it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6210a8fd-e48b-4f1a-bb79-90b32a2b8600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_value_function(V, title=\"Value Function\"):\n",
    "    # TODO: Reuse your `plot_value_function()` from the Lab02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ee94ba-1aa7-4d4c-bae2-2cb563416177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate your value function based on your action value function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8773d7d-b1ed-4746-b888-b51e8338d845",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_value_function(..., title=\"Optimal Value Function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1edab8-8f5c-4826-83d7-30f0b4fb5556",
   "metadata": {},
   "source": [
    "## 3.3 Visualizing the policy\n",
    "\n",
    "Come up with a creative way to visualize the learned policy. Keep in mind that your policy is different for games with an ace and without the ace. Here is just one possible way of visualizing the policy:\n",
    "\n",
    "![policy](images/Ex3.3_policy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320e3edf-45f7-4218-b715-c82d25ff1414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize your learned policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c6feef-20c9-4e84-807a-ec1bed911037",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

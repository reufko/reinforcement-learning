{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f46c6e8a-5345-484d-a33e-255bc2ca0270",
   "metadata": {},
   "source": [
    "# Lab03 - Monte Carlo Control\n",
    "\n",
    "### Learning Goals:\n",
    "- Understanding policies in the context of Reinforcement Learning\n",
    "- Understanding Monte Carlo methods and implementing a First-visit MC prediction algorithm\n",
    "- Visualizing the value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e71e89-251a-44b3-9561-51405f894e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "from toolbox.blackjack import BlackjackEnv\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d25218b-f029-463d-994a-88828d7c9c43",
   "metadata": {},
   "source": [
    "## 3.1 Monte Carlo Control\n",
    "\n",
    "Let's consider how Monte Carlo estimation can be used to approximate optimal policies. The overall idea is to maintain both an approximate policy and an approximate value function. The value function is repeatedly altered to more closely approximate the value function for the current policy, and the policy is repeatedly improved with respect to the current value function. \n",
    "\n",
    "To begin let's recall the classical policy iteration. In this method, alternating complete steps of policy evaluation and policy improvement are completed:\n",
    "\n",
    "\\begin{equation*}\n",
    "   \\large \\pi_0 \\xrightarrow[]{E} v_{\\pi_0} \\xrightarrow[]{I} \\pi_1 \\xrightarrow[]{E} v_{\\pi_1} \\xrightarrow[]{I} \\pi_2 \\xrightarrow[]{E} ... \\xrightarrow[]{I} \\pi_* \\xrightarrow[]{E} v_*\n",
    "\\end{equation*}\n",
    "\n",
    "In the Monte Carlo version of classical policy iteration policy evaluation is done exactly like in the preceding section. Many episodes are experienced,  with the approximate action-value function approaching the true function asymptotically. We assume that we observe an infinite number of episodes with exploring starts. Under these assumptions, the Monte Carlo methods will compute each $q_{\\pi_k}$ exactly, for arbitrary $\\pi_k$. \n",
    "\n",
    "Policy improvement is done by making the policy greedy with respect to the current value function. No model is needed, since we have an action-value function. For each $s \\in \\mathcal{S}$ we deterministically choose an action with maximal action-value:\n",
    "\\begin{equation}\n",
    "    \\pi(s) \\doteq \\underset{a}{\\operatorname{arg max}} q(s,a)\n",
    "\\end{equation}\n",
    "\n",
    "Two unlikely assumptions have been made in order to obtain the convergence guarantee. First one, episodes have exploring starts and second, policy evaluation can be done with an infinite number of episodes.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>MC Control:</b> Idea is the same as for Dynamic Programming. Use MC Policy Evaluation to evaluate the current policy then improve the policy greedily.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>The Problem:</b> How do we ensure that we explore all states if we don't know the full environment?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Solution to exploration problem:</b> Use epsilon-greedy policies instead of full greedy policies. When making a decision act randomly with probability epsilon. This will learn the optimal epsilon-greedy policy.\n",
    "</div>\n",
    "\n",
    "**TODO:** Implement the on-policy first-visit MC control (for $\\epsilon$-soft policies) from Sutton & Barto Chapter 5.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ca64ca-865a-4344-8d8a-59e9f1d8c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlackjackEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286b52d3-f1c9-41d0-a5ba-fffd268a3855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_epsilon_greedy(env, num_episodes, discount_factor=1.0, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Monte Carlo Control using Epsilon-Greedy policies.\n",
    "    Finds an optimal epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    "    \n",
    "    Returns:\n",
    "        Q is a dictionary mapping state -> action values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement the on-policy first-visit MC control (for  ùúñ -soft policies) from Sutton & Barto Chapter 5.4.\n",
    "   \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7dce33-b4df-4f5f-aa9e-2298f9d8d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = mc_control_epsilon_greedy(env, num_episodes=500000, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9aca56-cf5a-4f4f-8b7f-ec0f45ad6bcc",
   "metadata": {},
   "source": [
    "## 3.2 Visualizing the Value Function\n",
    "\n",
    "Your function `mc_control_epsilon_greedy()` returns the action-value function $q_\\pi(s,a)$ (`Q`) for the learned $\\epsilon$-greedy policy. Use the generated `Q` to calculate the value function:\n",
    "\n",
    "\\begin{equation*}\n",
    "   \\large v(s) = \\max_a q_\\pi(s,a)\n",
    "\\end{equation*}\n",
    "\n",
    "**TODO:** Calculate the value function and reuse the `plot_value_function()` from the Lab02 to visualize it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6210a8fd-e48b-4f1a-bb79-90b32a2b8600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_value_function(V, title=\"Value Function\"):\n",
    "    \"\"\"\n",
    "    Plots the value function as a surface plot.\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8773d7d-b1ed-4746-b888-b51e8338d845",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_value_function(V, title=\"Optimal Value Function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1edab8-8f5c-4826-83d7-30f0b4fb5556",
   "metadata": {},
   "source": [
    "## 3.3 Visualizing the policy\n",
    "\n",
    "Come up with a creative way to visualize the learned policy. Keep in mind that your policy is different for games with an ace and without the ace. Here is just one possible way of visualizing the policy:\n",
    "\n",
    "![policy](images/Ex3.3_policy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "320e3edf-45f7-4218-b715-c82d25ff1414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize the policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

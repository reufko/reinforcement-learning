{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f46c6e8a-5345-484d-a33e-255bc2ca0270",
   "metadata": {},
   "source": [
    "# Lab01 - Policy Iteration and Value Iteration using Dynamic Programming\n",
    "\n",
    "### Learning Goals:\n",
    "- Getting to know the gridworld environment and basic operations\n",
    "- Understanding policies in the context of Reinforcement Learning\n",
    "- Understanding and implementing policy evaluation for a given policy\n",
    "- Understanding and implementing policy and value iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1f3c26fa-f81c-44df-9982-dd8ff2785d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "87d731e6-482d-451f-8c3f-b0011ab07bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1c1a8fa8-1af1-4de8-9540-e8f69b3cc9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolbox.gridworld import GridworldEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d6f9c2cd-6d3f-4937-b1be-dbf28fe14b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dcd1d510-f0a2-4341-b353-f14a157ae280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env._render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5a080224-9ca1-4282-9046-37c773c9003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3d43ff-b124-4b6c-9831-59e8e1fbaaac",
   "metadata": {},
   "source": [
    "## 1.1 Policy Evaluation\n",
    "Computing the state-value function $v_\\pi$ for an arbitrary policy $\\pi$ is in reinforcement learning literature known as **policy evaluation**, sometimes also referred to as prediction problem. \n",
    "For all $s \\in \\mathcal{S}$:\n",
    " \\begin{align}\\label{eq:v_pi}\n",
    "    \\large v_\\pi(s) & \\doteq \\mathbb{E}[G_t | S_t = s] \\nonumber \\\\\n",
    "    & = \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} | S_t = s] \\nonumber \\\\\n",
    "    & = \\mathbb{E}[R_{t+1} + \\gamma v_{\\pi}(S_t+1) | S_t = s] \\nonumber\\\\\n",
    "    & = \\sum_a{\\pi(a|s)} \\sum_{s',r} p(s',r|s,a)\\big[r+\\gamma v_\\pi(s') \\big]\n",
    " \\end{align}\n",
    " $\\pi(a|s)$ is the probability of taking action $a$ in state $s$ under policy $\\pi$. \n",
    " \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Policy Evaluation</b>: Calculates the state-value function V(s) for a given policy.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16299df-1ee3-4e1b-9031-299a58548198",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "Consider the $ 4 \\times 4 $ gridworld shown below. \n",
    "\n",
    "![gridworld](images/Ex1.1_gridworld.png)\n",
    "\n",
    "The nonterminal states are $\\mathcal{S} = \\{1 , 2 , . . . , 14\\}$. There are four actions possible in each\n",
    "state, $\\mathcal{A} = \\{up, down, right, left\\}$, which deterministically cause the corresponding\n",
    "state transitions, except that actions that would take the agent off the grid in fact leave the state unchanged. \n",
    "Thus, for instance, $p(6,-1|5,right) = 1$, $p(7,-1|7,right) = 1$, and $p(10,r|5,right) = 0$ for all $r \\in \\mathcal{R}$.\n",
    "\n",
    "This is an undiscounted, episodic task. The reward is $-1$ on all transitions until the terminal state is reached. The terminal state is shaded in the figure (although it is shown in two places, it is formally one state). The expected reward function is thus $r(s,a,s')=-1$ for all states $s,s'$ and actions $a$. Suppose the agent follows the equiprobable random policy (all actions equally likely).\n",
    "\n",
    "**TODO:**\n",
    "Implement the **Iterative Policy Evaluation** algorithm from Sutton & Barto Chapter 4.1 in order to evaluate the current equiprobable policy and display the state-value function afterwards:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "20daf6e6-45f5-4b42-afb2-da4ba3dd1e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        # TODO: Implement iterative policy evaluation\n",
    "        break\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0ebf2d8a-8f6a-4693-b408-b3a03d8a9fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Declare and initialize a random equiprobable policy of a shape (number of states, number of actions) \n",
    "random_policy = ...\n",
    "\n",
    "# Calculate the approximate of the value function using the implemented function \n",
    "v = policy_eval(random_policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92b140b-8ce1-4c6d-a1de-07ca04b33c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(v.reshape(env.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a4fda2-1f0c-4194-ae59-6b22e6a20600",
   "metadata": {},
   "source": [
    "## 1.2 Policy Iteration\n",
    "In order to do policy iteration we first have to discuss **policy improvement**. Supposing we have the value function $v_\\pi$ for an arbitrary deterministic policy $\\pi$. For some state $s$ we would like to know whether we should choose an action which is not given by the current policy $a \\neq \\pi(s)$. One way to do it would be to take an action $a$ and follow the policy  $\\pi$ thereafter. The value of this could be computed by:\n",
    " \\begin{align}\n",
    "     \\large q_{\\pi}(s,a) & \\doteq \\mathbb{E}[R_{t+1} + \\gamma v_{\\pi}(S_t+1) | S_t = s] \\nonumber\\\\\n",
    "    & = \\sum_{s',r} p(s',r|s,a)\\big[r+\\gamma v_\\pi(s') \\big]\n",
    " \\end{align}\n",
    "If the value of taking the action $a$ and following the given policy thereafter is greater than only following the given policy then the policy $\\pi'$ must be as good as or better than $\\pi$:\n",
    "\\begin{equation}\n",
    "    \\large q_\\pi(s,\\pi'(s)) \\geq v_\\pi(s)\n",
    "\\end{equation}\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Policy Improvement:</b> Given the correct state-value function for a policy we can act greedily with respect to it (i.e. pick the best action at each state). Then we are guaranteed to improve the policy or keep it fixed if it's already optimal.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc0c025-76ab-4a92-a1ee-aec4d0a747ba",
   "metadata": {},
   "source": [
    "Once we improved the policy $\\pi$ to $\\pi'$, we can compute the new value function for the computed policy $v_{\\pi'}$. Hence, we can obtain a sequence of monotonically improving policies and value functions:\n",
    "\\begin{equation}\n",
    "   \\large \\pi_0 \\xrightarrow[]{E} v_{\\pi_0} \\xrightarrow[]{I} \\pi_1 \\xrightarrow[]{E} v_{\\pi_1} \\xrightarrow[]{I} \\pi_2 \\xrightarrow[]{E} ... \\xrightarrow[]{I} \\pi_* \\xrightarrow[]{E} v_*\n",
    "\\end{equation}\n",
    "This way of finding an optimal policy is called **policy iteration**.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Policy Iteration:</b> Iteratively perform Policy Evaluation and Policy Improvement until we reach the optimal policy.\n",
    "</div>\n",
    "\n",
    "**TODO:**\n",
    "Reuse the gridworld environment and already implemented policy evaluation to implement the **Policy Iteration** algorithm from Sutton & Barto Chapter 4.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "590b5370-0436-4c36-b9bf-0e65fa1a96fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, policy_eval_fn=policy_eval, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Policy Improvement Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI environment.\n",
    "        policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Start with a random policy\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    \n",
    "    while True:\n",
    "        # TODO: Implement policy improvement algorithm\n",
    "        break\n",
    "    \n",
    "    return policy, np.zeros(env.nS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f07fa287-bedb-4aec-8aa8-e940c48f8fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, v = policy_improvement(env)\n",
    "\n",
    "# TODO: Print policy probability distributions\n",
    "\n",
    "# TODO: Print the new value function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653e183f-2edf-4f37-bcae-b3bcf777ae78",
   "metadata": {},
   "source": [
    "## 1.3 Value Iteration\n",
    "Policy iteration includes a policy evaluation in each of its iterations. If policy evaluation is done iteratively, then convergence exactly to $v_\\pi$ occurs only in the limit. But we do not always have to wait for exact convergence, this process could be stopped earlier. \n",
    "\n",
    "One important special case is when policy evaluation is stopped after just one sweep (one update of each state). This algorithm is called **value iteration**. It can be written as a particularly simple update operation that combines the policy improvement and truncated policy evaluation steps:\n",
    "\\begin{align}\n",
    "    \\large v_{k+1}(s) &\\doteq \\max_a \\mathbb{E}[R_{t+1} + \\gamma v_k(S_{t+1}) | S_t = s, A_t = a] \\nonumber \\\\\n",
    "    &= \\max_a \\sum_{s',r} p(s',r|s,a)[r+\\gamma v_k(s')], \\text{for all } s \\in \\mathcal{S}\n",
    "\\end{align}\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Value Iteration:</b> Instead of doing multiple steps of Policy Evaluation to find the \"correct\" V(s) we only do a single step and improve the policy immediately.\n",
    "</div>\n",
    "\n",
    "**TODO:** \n",
    "Implement the **Value Iteration** algorithm from Sutton & Barto Chapter 4.4 using the same gridworld environment and display the new policy afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8d73c453-8d6c-4cb6-b236-0702f8f6f086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V) of the optimal policy and the optimal value function.        \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    V = np.zeros(env.nS)\n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    \n",
    "    # TODO: Implement the value iteration algorithm\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "016b449d-3515-4cc3-9ad6-22c0d8de5e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, v = value_iteration(env)\n",
    "\n",
    "# TODO: Print policy probability distributions\n",
    "\n",
    "# TODO: Print the new value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ca64ca-865a-4344-8d8a-59e9f1d8c4d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
